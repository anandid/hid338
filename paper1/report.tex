\documentclass[sigconf]{acmart}

\usepackage{hyperref}

\usepackage{endfloat}
\renewcommand{\efloatseparator}{\mbox{}} % no new page between figures

\usepackage{booktabs} % For formal tables

\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers

\begin{document}
	\title {Docker in Support of Big Data Applications and Analytics}
	
	
	\author{Anand Sriramulu}
	\orcid{1234-5678-9012}
	\affiliation{%
		\institution{Indiana University}
		\streetaddress{107 S Indiana Ave}
		\city{Bloomington} 
		\state{Indiana}
		\country{USA}
		\postcode{47405}
	}
	\email{asriram@iu.edu}
	
	
	% The default list of authors is too long for headers}
	\renewcommand{\shortauthors}{Anand S}
	
	
	\begin{abstract}
		To Discuss on the docker benefits in areas of \textit{Continuous Deployment} and Testing, Security, Isolation, Multi-Cloud Platform and Environment Standardization, and explaining different use cases in which docker can improve the performance of Big Data applications.
	\end{abstract}
	
	\keywords{i523, hid338, Data Science, Docker, Containers, Big Data Analytics, Cloud Computing}
	
	\maketitle
	
	\section{Introduction}
	Big data growing rapidly as the industries deal with large datasets in terms of Terabytes or Petabytes, in which we need for better solutions in the software development. 	
	Docker is a open source software containerization platform provides solutions for developers and sysadmins to build ship and run distributed applications whether on laptops, data centers,virtual machine or on the cloud.
	Docker provides lightweight environment that makes it easy to quickly deploy a piece of Software and the resources such as CPU, memory, disk, etc and make it portable and self-contained. \cite{WhatDocker}

	\section{Docker Benefits}
	Docker is a widely used container and it's being very matured compared to the others. I have outlined the top five benefits of using the ever-growing platform. \cite{DockerBenefits}
	\subsection{Continuous Deployment and Testing}
	\textit{Continuous Deployment} is a DevOps process in which the applications or features from continuous integration and deployed to production environment.
	Docker helps both development and devops and make sure it's consistency across environments. 
	
	Docker containers can be configured with all the configurations and dependencies internally, so that the developer need not worry about the environment as he can develop or test any product upgrades, maintenance releases, new features in a docker container and release the images to different production servers. This is a great advantage as it saves lot of time with no errors due to the deployment issues.\cite{Isolation}
	 
	\subsection{Multi-Cloud Platforms}
	Docker being widely used container solution, all the cloud computing provides supports it, so the portability being greatest strength with docker.
	That means, the docker image running on AWS can be switched to Azure server easily as the applications built on the docker container is not depending on any platform. This gives the advantage for the application to free of Platform As a Service vendor lock and provides the level of abstraction from the infrastructure layer.
	List of hosting provides supporting docker includes AWS, Microsoft Azure, Digital Ocean, Exoscale, Google Compute Engine, OpenStack, Rackspace, IBM Softlayer, etc.
	\cite{MultiCloudPlatform}
	\subsection{Environment Standardization and Version Control}
	Docker support version control as like GIT or TFS repositories. The docker images can be version controlled and if there any issues in the deployment, it can be roll-backed to the previous version. The process of rollback is quick and easy when compared to VM backup and image creation processes.\cite{Isolation}
	\subsection{Isolation}
	
	Since docker is a container, it will be isolated from other containers and resources in the same host. Docker also make sure each container has it's own resource been allocated and isolated from the other containers. This gives the benefit on each container can run on its own application stack and be managed. So if an application is not needed, it can be removed by deleting the container and it will not leave any temporary or container related files on the host system.
	As mentioned earlier, each container has assigned with allocated resource, the docker make sure that it will not be exceeded. This prevents the issues related to the performance or down time of the other applications in the same host.\cite{Isolation}
	\subsection{Security}
	
	As the containers are isolated, Docker make sure that the applications that are running on containers have control only within their container. So no container can look into the processes of other container. Each container will have its own resources ranging from processing to network stacks which is great benefit as if there any impact to an application related to security it won't impact the other applications.
	
	\cite{DockerSecurity}
	
	
	\section{Bigdata and Docker}
	
	Big Data is one of the big trends in IT of recent years. Majority of companies are investing more time in collecting and managing data for the business needs.	
	It's huge struggle for them to find a right system to get the relevant information needed for the business managers to make important decisions.	
	Without big data, there are challenges to arm the organization with the technology stack, skilled professional and resources for the business intelligence to manage the data deluge.
	\cite{bluedatablog}
	\subsection{Use Docker To Avoid Dependency Issues}
	
	Each developer might have different set of big data tools and not to mention all the dependencies required, which then must be distributed to each machine in a cluster.
	
	Companies assume this situation is manageable, but get enough developers on the same cluster and there are high possible chances for one tools requirements to break another. This will cause all the dependencies issues.
	
	In this scenario, the companies either need to get the entire entire development team to use the common technology stack, or use Docker. Docker allows the developers to build each applications to be self contained with their dependencies. This gives the benefit to have different applications can run without a conflict.\cite{AvoidDependency}
	
	\subsection{Reduce Reliance On MapReduce Experts With Pachyderm}
	
	Hadoop with MapReduce been popular for the distributed storage and big data framework. Pachyderm claims to be the modern Hadoop which uses Docker containers and Kubernetes for managing the clusters. Pachyderm Filesystem and Pachyderm Pipeplines are equivalent to HDFS and MapReduce respectivevly.
	
	Hadoop relies on Java technology stack, in which it requires specialist programmers to build MapReduce job, in which Pachyderm gives the freedom for the programmers to choose any library and wrap it in the container for the data processing and integrate into the Pachyderm stack. \cite{pachyderm}
		
	\subsection{Run Scheduled Analytics Using Containers With Chronos}
	The containers are a great way of deploying services at scale and giving isolation to services that run on the same host and improving utilization, but Docker can also be used for batch processing as well.
	
	Chronos job scheduler provides a graphical user interface which allows the devops to run the Docker images into a Mesos cluster. This gives the benefit that the developers can use containers to run the scheduled data analytics.\cite{Chronos}
	
	Chronos also gives the advantage that it doesn't need any manual setup on the cluster nodes to distribute the job processing in the containers.
	
	\subsection{Provision A Big Data Dev Environment Using Ferry}
	Ferry allows you to create big data clusters on the local machine (and AWS). The beauty of Ferry is that it allows anyone to define a big data stack using YAML, and then share it with other developers using a Dockerfile.
	As per the article \cite{Ferry}, "Setting up a Hadoop cluster is as simple as:
	
	
	\begin{itemize}
		\item[]\noindent backend:
		\item[] - storage 
		\item[] personality: 'hadoop'
		\item[] instances: 2
		\item[] layers:
		\item[] - 'hive'
		\item[]\noindent Connectors: 
		\item[] - personality: 'hadoop-client'
	\end{itemize}	
	
	Get started by typing
	
	\begin{itemize}
		\item[] ferry start hadoop"
	\end{itemize}	
	
	
	This will create a two node Hadoop cluster and a single Linux client. This can be customized at runtime or defined using a Dockerfile.
	Ferry is great for developers who want to get up and running with a big data environment using a test AWS box, developers that need a local big data dev environment, or users that want to share Big Data applications.
	
	Running Ferry on AWS also has several advantages over something like Elastic MapReduce, such as not tying you to a single cluster of a single type (such as Hadoop).\cite{Ferry}
	
	\subsection{Run Big Data As Microservices With Coho}
	
	Large enterprise applications use publish-subscribe technology as part of the SOA Architecture can take the advantage of implementing each component as Microservice which solves a single purpose.
	
	Microservice can help big data systems in terms of scalability(as each component runs independently), data quality( as data flows through the focused task to run the data analytics), Muliple Technology Stack.
	
	Coho Data's DataStream Storage system provides the mechanism in which the docker containers can run on the data environment without any other needed infrastructure.
	The HDFS users can run the HDFS containers directly on the storage envrionment to extract the information needed for the data analytics\cite{Coho}
	
	\section{Conclusions}
	
	The complex nature of big data and the tools used to analyze these data sets makes efficient processing difficult with standard environments. 
	
	While performance and pipeline efficiency were key components of this implementation, Docker containers also allow for application isolation from the host operating system. Since many big data tools have complex sets of dependencies and are difficult to build from source, the ability to deploy containers with different operating systems and dependency versions to the same host decreases the amount of effort needed to being analysis. With the use of containers allowed the deployment of each utility on its natively supported operating system, which improves stability and decreases the potential for dependency conflicts among software applications.
	There are other tools like Kubernetes or Docker Swarm can be used for container orchestration and helps us to distribute the containers in the clusters, but these applications will not provide application level workflows as it works within the container level. Additional implementation experience about the use of these tools within high-performance clusters may provide valuable insights about the scalability of these tools within data analytics workflows.
	Finally, the container technology is very helpful to deploy the applications to nearly any host system. While many factors can impact reproducibility, the use of containers limits variability due to differences in software environment or application configuration when appropriately deployed. The continued use of emerging technology and novel approaches to software architecture has the potential to increase the efficiency of computational analysis in big data. \cite{MongoDbWhitePaper}

\begin{acks}		
	
	The author would like to thank Dr. Gregor von Laszewski and the Teaching Assistants for their support and valuable suggestions.
	
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 

\end{document}
